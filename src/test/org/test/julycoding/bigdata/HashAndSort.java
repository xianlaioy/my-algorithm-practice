package org.test.julycoding.bigdata;

/**
 * 问题一：海量日志数据，提取出某日访问百度次数最多的那个IP.
 *  【思路】：
 *      - 1.1 将访问日志中的ip提取出来存放到一个大文件里，该文件中的数据仍然是海量，无法一次导入
 *      内存进行统计；先通过hash，使用hash函数计算ip的值，然后将hash值%1000，则相同的ip被映射到同一个
 *      小文件中，将则所有的ip映射到1000个小文件中了；(hash(ip)%1000)；
 *      - 1.2  分别将这1000个小文件读入内存，使用HashMap统计ip的次数，求出每一个小文件中ip出现次数最多
 *      的ip及次数，汇总；
 *      - 1.3 从这1000个ip中求出出现次数最多的ip；
 *
 * 问题二：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。
 * 假设目前有一千万个记录，请你统计最热门的10个查询串，要求使用的内存不能超过1G。
 *  【思路】：
 *      - 2.1 先分析需要的内存，一千万条记录，每个串的长度为1-255字节，最大占用内存可能为2.55G；也是先
 *      通过hash，将日志文件映射到小文件中，比如可以为10个；
 *      - 2.2 分别统计这10个小文件中字符串的出现频率，取每个文件中出现频率最高的10个串；
 *      - 2.3 将这10*10=100个字符串汇总，并计算出频率最高的10个串；(如果快速排序，复杂度为O(nlogn)，如果
 *      使用堆排序，维护一个top10的最小堆(对顶为最小值)，则复杂度为(nlogk))；
 *  【扩展】：如果串的重复度比较高，总数虽然有一千万，但去除重复后，只有三百万，则可以一次性读入内存，剩下的
 *  就是求topk的问题了，维护一个有k个元素的最小堆，复杂度为O(nlogk)；
 *  【总结】：对于求topk的问题，考虑使用堆排序，如果要求的是最大的k个值，则应该使用最小堆，堆顶为最小值；
 *  如果要求最小的k个值，则使用最大堆，堆顶为最大值；
 *
 * 问题三：有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最
 * 高的100个词。
 *  【思路】：
 *      - 仍然采用hash分成2000个小文件，每一个小文件差不多500k，如果有小文件超过1M，则继续分解，直到大小在1M
 *      的限制以内；分别统计top100，然后汇总，使用最小堆，计算最后的top100即可；
 *
 * 问题四：给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？
 *  【思路】：
 *      - 单个文件大小：5G*64=320G，所以文件必须拆分；选择一个hash函数，对url进行hash，然后将这些url分到200
 *      个小文件中(如%200)，则每个小文件大小约为1.6G，a、b两个文件使用相同的hash函数，并将各自的小文件命令为：
 *      a1,a2,...a200; b1,b2,...,b200，则a、b两个文件中相同的url必定位于同样编号的小文件中；
 *      - 对每一对小文件，求共同的url，可以使用HashSet，即将一个小文件的所有url存到一个HashSet，然后遍历对应
 *      的小文件，如果这个url在HashSet中存在，则为共同的url，存到最终的结果文件中即可；
 *
 * 问题五：100万个数中找出最大的100个数。
 *  【思路】：
 *      - 堆排序，使用前100个元素构成一个最小堆，堆顶为最小元素，遍历剩下的元素，如果比堆顶小，则忽略，否则
 *      替换掉堆顶，堆调整；复杂度O(n*logk)，其中n=10^6，k=100；
 *
 *
 *
 *
 *
 *
 *
 * @author: lingguo
 * @time: 2014/8/31 15:46
 */
public class HashAndSort {
}
